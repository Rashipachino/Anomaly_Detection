{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task -1 Anomaly detection\n",
    "## Student ID1: 345174478\n",
    "## Student ID2: 326876786"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment we will be using the Isolation Forest method to detect anomalies among the given dataset. \n",
    "#### In the following report, we have explored the data, answered the assignent questions and trained and tested our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install oletools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path - this for linux windows you will need \"//\"\n",
    "f_path = \"conn_attack.csv\"\n",
    "'''\n",
    "record ID - The unique identifier for each connection record.\n",
    "duration_  This feature denotes the number of seconds (rounded) of the connection. For example, a connection for 0.17s or 0.3s would be indicated with a “0” in this field.\n",
    "src_bytes This field represents the number of data bytes transferred from the source to the destination (i.e., the amount of out-going bytes from the host).\n",
    "dst_bytes This fea\n",
    "ture represents the number of data bytes transferred from the destination to the source (i.e., the amount of bytes received by the host).\n",
    "'''\n",
    "df = pd.read_csv(f_path,names=[\"record ID\",\"duration_\", \"src_bytes\",\"dst_bytes\"], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "### Here we have explored the data in order to gain a further understanding of the features. \n",
    "##### Comments on what we learned from this are written throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationship with numerical variables\n",
    "var = 'record ID'\n",
    "data = pd.concat([df['src_bytes'], df[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='src_bytes', ylim=(0,60000)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationship with numerical variables\n",
    "var = 'record ID'\n",
    "data = pd.concat([df['dst_bytes'], df[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='dst_bytes', ylim=(0,800000)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationship with numerical variables\n",
    "var = 'record ID'\n",
    "data = pd.concat([df['duration_'], df[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='duration_', ylim=(0,1600)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By graphing the record ID against the src_bytes, dst_bytes and duration_ features, it is simplier to see which instances are anomalous. Record ID will not provide the model with any additional information about the data, and can therefore be disregarded as a feature when training the model. However, each graph produces outliers, meaning through any one feature alone (src_bytes, dst_bytes and duration_), anomalies can be detected and thus each of these features are vital for the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['dst_bytes'])\n",
    "print(\"Skewness: %f\" % df['dst_bytes'].skew())\n",
    "print(\"Kurtosis: %f\" % df['dst_bytes'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['duration_'])\n",
    "print(\"Skewness: %f\" % df['duration_'].skew())\n",
    "print(\"Kurtosis: %f\" % df['duration_'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['src_bytes'])\n",
    "print(\"Skewness: %f\" % df['src_bytes'].skew())\n",
    "print(\"Kurtosis: %f\" % df['src_bytes'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The skewness tells us that there are a higher number of datapoints having lower src_byte, dst_byte and duration_ values. So when we train our model using Isolation Forest, data points with higher values for these features will be isolated quicker. \n",
    "##### Through the skewness and kurtosis of each feature we learn the direction of the outliers. Since all three features are positively skewed, most of the outliers will be present on the right side of the distribution. It does not tell us the number of outliers, rather the direction alone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the size of the heatmap.\n",
    "plt.figure(figsize=(16, 6))\n",
    "# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n",
    "# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\n",
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The correlation heatmap provides the correlation between any two features. Training a model using features that are heavily correlated is redundant. Therefore, correlation mapping is usually helpful in ridding one feature of a pair of correlated features. However, we can learn from this heatmap that our features are not highly correlated, so using all three is necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing an Unsupervised Model\n",
    "##### After exploring the data, we decided on the unsupervised model Isolation Forest. Since the dataset file does not contain labels, (we we're only given labels in a seperate file to check our work) the unsupervised algorithms group is suitable for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Isolation Forest?\n",
    "##### Isolation forest can identify anomalous datapoints using isolation rather than modeling the normal ones. After examining the data and noticing clear outliers within the graphs, we realized that instead of constructing a profile of what's \"normal\", and then report anything that cannot be considered normal as anomalous, our algorithm should explicitely isolate anomalous points in the dataset. The model processes data in a tree structure based on randomly selected features. All three features add context to the model and are thus helpful when trying to isolate data points through cuts. Data points with deeper trees are less likely to be anomalies since they required more cuts to isolate them. Furthermore, data points with shorter branches indicate anomalies as it was easier for the tree to separate them from other data points. We can observe that our model is sufficient through examining the confusion matrix, precision score and recall score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model\n",
    "##### In this section, we train, test and validate our results with the labels file. Isoltation Forest works as an ensemble of isolation trees. We chose 1000 base estimators using 256 samples each. Comments are written throughout the code along with a confusion matrix at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"conn_attack.csv\"\n",
    "df = pd.read_csv(DATA_PATH, header=None, names=[\"record ID\",\"duration_\", \"src_bytes\",\"dst_bytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns=[\"record ID\"], axis=1).copy() # removing record ID from the feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data # showing that record ID was indeed dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# max features is 3 since all 3 features are useful. \n",
    "model = IsolationForest(contamination=float(0.004), n_estimators=500, max_samples=256, max_features=3)\n",
    "model.fit(data.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model on the dataset and adding prediction as column\n",
    "df[\"is_anomaly?\"] = pd.Series(model.predict(data.values))\n",
    "df[\"is_anomaly?\"] = df[\"is_anomaly?\"].map({1: 0, -1: 1}) # instead of 1:normal -1:anomaly, we mapped to 0:normal 1:anomaly\n",
    "print(df[\"is_anomaly?\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df.drop([\"duration_\",\"src_bytes\",\"dst_bytes\"], axis=1) # dropping the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing that the prediction results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"conn_attack_iforest_pred.csv\", index=False) # output file with prediction column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating the model against the labels given\n",
    "PATH_TO_LABELS = 'conn_attack_anomaly_labels.csv'\n",
    "data_labels = pd.read_csv(PATH_TO_LABELS, header=None, names=[\"record ID\",\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix, Accuracy and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(df[\"is_anomaly?\"], data_labels[\"label\"], labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(data_labels[\"label\"], df[\"is_anomaly?\"])\n",
    "print(\"accuracy score: {0:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(data_labels[\"label\"], df[\"is_anomaly?\"])\n",
    "print(\"recall score: {0:.2f}%\".format(recall*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "##### Our model has an accuracy score of 99.97% and recall score of 96.87%. The high precision score outlines the model's ability to accurately identify anomalous data. Meaning, from all the data points the model classified as anomalous, most of them were true positives. As we are dealing with anomaly detection, recall is important. Recall is the measure of predicted anomalies over the total number of anomalous data points. Emphasizing a higher recall rate means clients would prefer the false negative number to be as low as possible. This is important in anomaly detection because of the possibilty of cyber attacks that can evolve from undiagnosed anomalous data points. Therefore, we can presume that our model is satisfactory. \n",
    "### Link to Github : https://github.com/Rashipachino/Anomaly_Detection.git\n",
    "### How to run : \n",
    "#### 1. Open terminal at folder containing the Dockerfile. \n",
    "#### 2. Run the following commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t iforest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -t -d -p 8080:8080 iforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server will be running at: http://0.0.0.0:8080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
